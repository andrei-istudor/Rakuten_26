{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d0cc40",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b17a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import os, json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import evaluate\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8081e3b6",
   "metadata": {},
   "source": [
    "# Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd921f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Project paths\n",
    "# -----------------------------\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_PATH_train = DATA_DIR / \"train_fixed.csv\"\n",
    "DATA_PATH_val = DATA_DIR / \"test_fixed.csv\"\n",
    "EXPERIMENTS_DIR = Path(\"experiments\")\n",
    "EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "019f7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Dataset column config\n",
    "# -----------------------------\n",
    "\n",
    "# TEXT_VARIANT_COL  = \"text_english\"  \n",
    "# TEXT_VARIANT_COL  = \"text_stripped_lowercase\" \n",
    "TEXT_VARIANT_COL  = \"text_stripped\" \n",
    "LABEL_COL = \"prdtypecode\"\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e82eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Configuration of tokenizer, model, training\n",
    "# -----------------------------\n",
    "\n",
    "CFG = {\n",
    "    \"experiment_id\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),  # unique run id\n",
    "    \"text_col\": TEXT_VARIANT_COL,          # text column to use for this run\n",
    "    \"label_col\": LABEL_COL,          # label column to use for this run\n",
    "    \n",
    "    \"seed\": RANDOM_STATE,                         # reproducibility\n",
    "    \"model_ckpt\": \"jhu-clsp/mmBERT-base\",  # model\n",
    "    \n",
    "    # Tokenizer parameters\n",
    "    \"max_length\": 256,                  # token length for padding/truncation later\n",
    "    \"padding\": False,         \n",
    "    \"truncation\": True,              # truncate sequences longer than max_length\n",
    "    \n",
    "    # imbalance handling\n",
    "    \"use_class_weights\": True,         # whether to use class weights in the loss function\n",
    "    \"class_weight_method\": \"inv_freq\",  # \"inverse\" or \"sqrt_inv\" for gentler weights\n",
    "    \"class_weight_eps\": 1e-6,        # to avoid division by zero\n",
    "    \n",
    "    # model\n",
    "    \"eval_strategy\": \"epoch\",   \n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_total_limit\": 2,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"f1_macro\",\n",
    "    \"greater_is_better\": True,\n",
    "    \"logging_steps\": 100,\n",
    "    \"report_to\": \"none\",\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \n",
    "    \"fp16\": False,\n",
    "    \"bf16\": True,                    \n",
    "    \"label_smoothing_factor\": 0.0,    # label smoothing factor    \n",
    "    \n",
    "    \"warmup_ratio\": 0.06,              # fraction of total steps used for warmup\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 2e-5, # learning rate\n",
    "    \"epochs\": 4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"greater_is_better\": True,\n",
    "    \"gradient_accumulation_steps\": 2,          \n",
    "}\n",
    "\n",
    "\n",
    "BASE_CFG = deepcopy(CFG)\n",
    "\n",
    "# EXPERIMENTS = [\n",
    "#     # -----------------------------\n",
    "#     # Reference: mmBERT\n",
    "#     # -----------------------------\n",
    "#     {\n",
    "#         \"run_name\": \"mmBERT_lr2e-5_bs32\",\n",
    "#         \"model_ckpt\": \"jhu-clsp/mmBERT-base\",\n",
    "#         \"lr\": 2e-5,\n",
    "#         \"batch_size\": 32,\n",
    "#     },\n",
    "\n",
    "#     # -----------------------------\n",
    "#     # XLM-RoBERTa\n",
    "#     # -----------------------------\n",
    "#     {\n",
    "#         \"run_name\": \"XLMR-base_lr2e-5_bs32\",\n",
    "#         \"model_ckpt\": \"FacebookAI/xlm-roberta-base\",\n",
    "#         \"lr\": 2e-5,\n",
    "#         \"batch_size\": 32,\n",
    "#     },\n",
    "\n",
    "#     # -----------------------------\n",
    "#     # Multilingual DeBERTa v3\n",
    "#     # -----------------------------\n",
    "#     {\n",
    "#         \"run_name\": \"mDeBERTa-v3_lr2e-5_bs32\",\n",
    "#         \"model_ckpt\": \"microsoft/mdeberta-v3-base\",\n",
    "#         \"lr\": 2e-5,\n",
    "#         \"batch_size\": 32,\n",
    "#     },\n",
    "\n",
    "#     # -----------------------------\n",
    "#     # Multilingual BERT (mBERT)\n",
    "#     # -----------------------------\n",
    "#     {\n",
    "#         \"run_name\": \"mBERT_lr2e-5_bs32\",\n",
    "#         \"model_ckpt\": \"google-bert/bert-base-multilingual-cased\",\n",
    "#         \"lr\": 2e-5,\n",
    "#         \"batch_size\": 32,\n",
    "#     },\n",
    "\n",
    "#     # -----------------------------\n",
    "#     # Distilled multilingual BERT\n",
    "#     # -----------------------------\n",
    "#     {\n",
    "#         \"run_name\": \"Distil-mBERT_lr2e-5_bs32\",\n",
    "#         \"model_ckpt\": \"distilbert/distilbert-base-multilingual-cased\",\n",
    "#         \"lr\": 2e-5,          \n",
    "#         \"batch_size\": 32,    \n",
    "#     },\n",
    "# ]\n",
    "\n",
    "LRS = [2e-5]\n",
    "BATCH_SIZES = [16]\n",
    "MAX_LENGTHS = [16,32, 64]\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    {\n",
    "        \"run_name\": f\"mmBERT_lr{lr}_bs{bs}_len{ml}\",\n",
    "        \"model_ckpt\": \"jhu-clsp/mmBERT-base\",\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": bs,\n",
    "        \"max_length\": ml,\n",
    "    }\n",
    "    for lr in LRS\n",
    "    for bs in BATCH_SIZES\n",
    "    for ml in MAX_LENGTHS\n",
    "]\n",
    "\n",
    "\n",
    "# Facebook AI for monolingual BERT models\n",
    "\n",
    "# EXPERIMENTS = [\n",
    "#     # -----------------------------\n",
    "#     # Reference: mmBERT\n",
    "#     {\n",
    "#     \"run_name\": \" mmBERT-base\",\n",
    "#     \"model_ckpt\": \"jhu-clsp/mmBERT-base\",\n",
    "#     \"lr\": 2e-5,\n",
    "#     \"batch_size\": 16,\n",
    "#     \"max_length\": 384,\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "def make_cfg(base, overrides):\n",
    "    cfg = deepcopy(base)\n",
    "    cfg.update(overrides)\n",
    "    cfg[\"experiment_id\"] = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    return cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b21bef",
   "metadata": {},
   "source": [
    "# Loading, tokenizing, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67076ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(CFG):\n",
    "\n",
    "    # output directory per run\n",
    "    OUT_DIR = f\"experiments/{CFG['model_ckpt'].split('/')[-1]}_{CFG['experiment_id']}\"\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    # Save config to json\n",
    "    with open(os.path.join(OUT_DIR, \"cfg.json\"), \"w\") as f:\n",
    "        json.dump(CFG, f, indent=2)\n",
    "\n",
    "    print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "    ##### read data frame ###\n",
    "    ##############################################\n",
    "\n",
    "    # splitting is already done\n",
    "    # train df\n",
    "    train_df = pd.read_csv(DATA_PATH_train)\n",
    "    # val df\n",
    "    val_df = pd.read_csv(DATA_PATH_val)\n",
    "\n",
    "    #### LabelEncoder ###\n",
    "    ##############################################\n",
    "\n",
    "    # print shapes\n",
    "\n",
    "    train_df = train_df[[CFG[\"text_col\"], CFG[\"label_col\"]]].astype(str)\n",
    "    val_df = val_df[[CFG[\"text_col\"], CFG[\"label_col\"]]].astype(str)\n",
    "\n",
    "    print(\"Train shape:\", train_df.shape)\n",
    "    print(\"Validation shape:\", val_df.shape)\n",
    "\n",
    "    # encoding labels\n",
    "    le = LabelEncoder()\n",
    "    train_df[\"label\"] = le.fit_transform(train_df[CFG[\"label_col\"]].astype(str))\n",
    "    val_df[\"label\"] = le.transform(val_df[CFG[\"label_col\"]].astype(str))\n",
    "\n",
    "    # print number of classes\n",
    "    num_classes = len(le.classes_)\n",
    "    print(\"Number of classes:\", num_classes)\n",
    "\n",
    "    # Save mappings for later interpretation / inference\n",
    "    label_names = list(le.classes_)  # index = label id\n",
    "    id2label = {i: label_names[i] for i in range(len(label_names))}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "\n",
    "    # save\n",
    "    with open(os.path.join(OUT_DIR, \"label_map.json\"), \"w\") as f:\n",
    "        json.dump({\"id2label\": id2label, \"label2id\": label2id}, f, indent=2)\n",
    "        \n",
    "        \n",
    "    ### Tokenization ###\n",
    "    ##########################################\n",
    "\n",
    "\n",
    "    from transformers import DataCollatorWithPadding\n",
    "\n",
    "    train_hf = Dataset.from_pandas(\n",
    "        train_df[[CFG[\"text_col\"], \"label\"]].reset_index(drop=True)\n",
    "    )\n",
    "    val_hf = Dataset.from_pandas(\n",
    "        val_df[[CFG[\"text_col\"], \"label\"]].reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(train_hf)\n",
    "    print(val_hf)\n",
    "\n",
    "\n",
    "    # 2) Load a tokenizer that matches your chosen checkpoint\n",
    "    #    Tokenizer turns text into token IDs the model understands.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG[\"model_ckpt\"], use_fast=False)\n",
    "\n",
    "    # 3) Define how to tokenize a batch of examples\n",
    "    def tokenize_batch(batch):\n",
    "        out = tokenizer(\n",
    "            batch[CFG[\"text_col\"]],   # list of texts\n",
    "            truncation=CFG[\"truncation\"],          # cut off texts longer than max_length\n",
    "            padding=CFG[\"padding\"],     # pad shorter texts to max_length\n",
    "            max_length=CFG[\"max_length\"],\n",
    "        )\n",
    "        out.pop(\"token_type_ids\", None)  # Deberta does not use token_type_ids\n",
    "        return out\n",
    "\n",
    "    # 4) Apply tokenization to the whole dataset\n",
    "    #    remove_columns removes the raw text column after tokenization to avoid duplication\n",
    "    train_tok = train_hf.map(tokenize_batch, batched=True, remove_columns=[CFG[\"text_col\"]])\n",
    "    val_tok   = val_hf.map(tokenize_batch, batched=True, remove_columns=[CFG[\"text_col\"]])\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    print(train_tok)\n",
    "    print(train_tok[0].keys())  # inspect produced fields\n",
    "    print(\"Example label:\", train_tok[1][\"label\"])\n",
    "    print(\"Example input_ids length:\", len(train_tok[0][\"input_ids\"]))\n",
    "\n",
    "\n",
    "    ### load label id mapping ###\n",
    "    ###########################################\n",
    "\n",
    "    with open(os.path.join(OUT_DIR, \"label_map.json\"), \"r\") as f:\n",
    "        maps = json.load(f)\n",
    "\n",
    "    id2label = {int(k): v for k, v in maps[\"id2label\"].items()}\n",
    "    label2id = {v: int(k) for v, k in maps[\"label2id\"].items()}  # invert back to int ids\n",
    "\n",
    "    num_labels = len(id2label)\n",
    "    print(\"num_labels:\", num_labels)\n",
    "    print(\"example:\", list(id2label.items())[:5])\n",
    "\n",
    "\n",
    "    ### Class weights ###\n",
    "    ##########################################\n",
    "\n",
    "\n",
    "    def compute_class_weights(train_df, label_col, mode=\"inv_freq\", eps=1e-6):\n",
    "        counts = (\n",
    "            train_df[label_col]\n",
    "            .value_counts()\n",
    "            .sort_index()\n",
    "            .values.astype(np.float32)\n",
    "        )\n",
    "\n",
    "        if mode == \"inv_freq\":\n",
    "            w = counts.sum() / (len(counts) * (counts + eps))\n",
    "        elif mode == \"sqrt_inv\":\n",
    "            w = np.sqrt(counts.sum() / (len(counts) * (counts + eps)))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown class_weight_mode: {mode}\")\n",
    "\n",
    "        w = w / w.mean()  # normalize → mean weight = 1\n",
    "        return torch.tensor(w, dtype=torch.float)\n",
    "\n",
    "    class_weights = None\n",
    "    if CFG[\"use_class_weights\"]:\n",
    "        class_weights = compute_class_weights(\n",
    "            train_df,\n",
    "            label_col=CFG[\"label_col\"],\n",
    "            mode=CFG[\"class_weight_method\"],\n",
    "            eps=CFG[\"class_weight_eps\"],\n",
    "        )\n",
    "        print(\"Class weights:\", class_weights)\n",
    "        \n",
    "        \n",
    "    ### Model ###\n",
    "    ###########################################\n",
    "\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        CFG[\"model_ckpt\"],     # e.g. \"distilbert-base-uncased\"\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id={v: k for k, v in id2label.items()}  # label string -> id\n",
    "    )\n",
    "\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        return {\n",
    "            \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "            \"f1_macro\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "        }\n",
    "\n",
    "    import torch.nn as nn\n",
    "\n",
    "    class OptionalWeightedTrainer(Trainer):\n",
    "        def __init__(self, class_weights=None, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.class_weights = class_weights\n",
    "\n",
    "        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "            labels = inputs.pop(\"labels\")\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            if self.class_weights is not None:\n",
    "                loss_fct = nn.CrossEntropyLoss(\n",
    "                    weight=self.class_weights.to(logits.device)\n",
    "                )\n",
    "            else:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUT_DIR,\n",
    "        seed=CFG[\"seed\"],\n",
    "        learning_rate=CFG[\"lr\"],\n",
    "        per_device_train_batch_size=CFG[\"batch_size\"],\n",
    "        per_device_eval_batch_size=CFG[\"batch_size\"],\n",
    "        num_train_epochs=CFG[\"epochs\"],\n",
    "        weight_decay=CFG[\"weight_decay\"],\n",
    "\n",
    "        eval_strategy=CFG[\"eval_strategy\"],   # <-- changed name (was evaluation_strategy)\n",
    "        save_strategy=CFG[\"save_strategy\"],\n",
    "\n",
    "        load_best_model_at_end=CFG[\"load_best_model_at_end\"],\n",
    "        metric_for_best_model=CFG[\"metric_for_best_model\"],\n",
    "        greater_is_better=CFG[\"greater_is_better\"],\n",
    "\n",
    "        logging_steps=CFG[\"logging_steps\"],\n",
    "        report_to=CFG[\"report_to\"],\n",
    "        \n",
    "        lr_scheduler_type=CFG[\"lr_scheduler_type\"],\n",
    "        fp16=CFG[\"fp16\"],\n",
    "        bf16=CFG[\"bf16\"],\n",
    "        \n",
    "        label_smoothing_factor=CFG[\"label_smoothing_factor\"],\n",
    "        gradient_accumulation_steps=CFG[\"gradient_accumulation_steps\"],\n",
    "        save_total_limit=CFG[\"save_total_limit\"],\n",
    "        warmup_ratio=CFG[\"warmup_ratio\"],\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=val_tok,\n",
    "        tokenizer=tokenizer,          # from your tokenization step\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    TrainerCls = OptionalWeightedTrainer if CFG[\"use_class_weights\"] else Trainer\n",
    "\n",
    "    trainer = TrainerCls(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=val_tok,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        class_weights=class_weights if CFG[\"use_class_weights\"] else None,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training using model:\", CFG[\"model_ckpt\"])\n",
    "    # print which Trainer is used\n",
    "    print(\"Using Trainer class:\", trainer.__class__.__name__)\n",
    "\n",
    "\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    train_out = trainer.train()\n",
    "    eval_out = trainer.evaluate()\n",
    "    print(eval_out)\n",
    "\n",
    "    # save eval metrics\n",
    "    with open(os.path.join(OUT_DIR, \"eval_metrics.json\"), \"w\") as f:\n",
    "        json.dump({k: float(v) for k, v in eval_out.items()}, f, indent=2)\n",
    "\n",
    "    # also save train metrics (loss curves etc.) if available\n",
    "    train_metrics = train_out.metrics if hasattr(train_out, \"metrics\") else {}\n",
    "    with open(os.path.join(OUT_DIR, \"train_metrics.json\"), \"w\") as f:\n",
    "        json.dump({k: float(v) for k, v in train_metrics.items()} if train_metrics else {}, f, indent=2)\n",
    "\n",
    "    print(\"Saved metrics to:\", OUT_DIR)\n",
    "\n",
    "    # This saves the \"best\" checkpoint if load_best_model_at_end=True\n",
    "    BEST_DIR = os.path.join(OUT_DIR, \"best_model\")\n",
    "\n",
    "    # also save train_tok, val_tok datasets\n",
    "    train_tok.save_to_disk(os.path.join(BEST_DIR, \"train_dataset\"))\n",
    "    val_tok.save_to_disk(os.path.join(BEST_DIR, \"val_dataset\"))\n",
    "\n",
    "    trainer.save_model(BEST_DIR)          # model + config\n",
    "    tokenizer.save_pretrained(BEST_DIR)   # tokenizer files\n",
    "\n",
    "    print(\"Saved best model to:\", BEST_DIR)\n",
    "    \n",
    "    return {\n",
    "        \"eval_f1_macro\": eval_out.get(\"eval_f1_macro\"),\n",
    "        \"eval_loss\": eval_out.get(\"eval_loss\"),\n",
    "        \"eval_accuracy\": eval_out.get(\"eval_accuracy\"),\n",
    "        \"best_model_checkpoint\": getattr(trainer.state, \"best_model_checkpoint\", None),\n",
    "        \"output_dir\": CFG.get(\"output_dir\", None),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9840d0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT_DIR: experiments/mmBERT-base_20260122_131611\n",
      "Train shape: (66800, 2)\n",
      "Validation shape: (16701, 2)\n",
      "Number of classes: 27\n",
      "Dataset({\n",
      "    features: ['text_stripped', 'label'],\n",
      "    num_rows: 66800\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text_stripped', 'label'],\n",
      "    num_rows: 16701\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 66800/66800 [00:06<00:00, 10013.50 examples/s]\n",
      "Map: 100%|██████████| 16701/16701 [00:01<00:00, 9435.38 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 66800\n",
      "})\n",
      "dict_keys(['label', 'input_ids', 'attention_mask'])\n",
      "Example label: 6\n",
      "Example input_ids length: 16\n",
      "num_labels: 27\n",
      "example: [(0, '10'), (1, '1140'), (2, '1160'), (3, '1180'), (4, '1280')]\n",
      "Class weights: tensor([0.6013, 0.7012, 0.4737, 2.4514, 0.3862, 0.9178, 0.3740, 2.6988, 0.7883,\n",
      "        0.5810, 0.3739, 0.4583, 2.3330, 0.3853, 2.2763, 0.3933, 0.3931, 1.3302,\n",
      "        0.3783, 0.7467, 0.1908, 0.7603, 0.6787, 2.1459, 0.7538, 1.1382, 2.2902])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at jhu-clsp/mmBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_15819/71275596.py:222: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/tmp/ipykernel_15819/71275596.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `OptionalWeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using model: jhu-clsp/mmBERT-base\n",
      "Using Trainer class: OptionalWeightedTrainer\n",
      "CUDA available: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8352' max='8352' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8352/8352 21:36, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.476800</td>\n",
       "      <td>0.769737</td>\n",
       "      <td>0.775103</td>\n",
       "      <td>0.757182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.963100</td>\n",
       "      <td>0.691882</td>\n",
       "      <td>0.802168</td>\n",
       "      <td>0.788206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.356200</td>\n",
       "      <td>0.846551</td>\n",
       "      <td>0.812167</td>\n",
       "      <td>0.799938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.025134</td>\n",
       "      <td>0.816897</td>\n",
       "      <td>0.807269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1044' max='1044' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1044/1044 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0251343250274658, 'eval_accuracy': 0.816897191784923, 'eval_f1_macro': 0.8072690333763385, 'eval_runtime': 20.0602, 'eval_samples_per_second': 832.542, 'eval_steps_per_second': 52.043, 'epoch': 4.0}\n",
      "Saved metrics to: experiments/mmBERT-base_20260122_131611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 66800/66800 [00:00<00:00, 577288.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 16701/16701 [00:00<00:00, 380686.99 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model to: experiments/mmBERT-base_20260122_131611/best_model\n",
      "OUT_DIR: experiments/mmBERT-base_20260122_133831\n",
      "Train shape: (66800, 2)\n",
      "Validation shape: (16701, 2)\n",
      "Number of classes: 27\n",
      "Dataset({\n",
      "    features: ['text_stripped', 'label'],\n",
      "    num_rows: 66800\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text_stripped', 'label'],\n",
      "    num_rows: 16701\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 66800/66800 [00:05<00:00, 12805.64 examples/s]\n",
      "Map: 100%|██████████| 16701/16701 [00:01<00:00, 13529.28 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 66800\n",
      "})\n",
      "dict_keys(['label', 'input_ids', 'attention_mask'])\n",
      "Example label: 6\n",
      "Example input_ids length: 20\n",
      "num_labels: 27\n",
      "example: [(0, '10'), (1, '1140'), (2, '1160'), (3, '1180'), (4, '1280')]\n",
      "Class weights: tensor([0.6013, 0.7012, 0.4737, 2.4514, 0.3862, 0.9178, 0.3740, 2.6988, 0.7883,\n",
      "        0.5810, 0.3739, 0.4583, 2.3330, 0.3853, 2.2763, 0.3933, 0.3931, 1.3302,\n",
      "        0.3783, 0.7467, 0.1908, 0.7603, 0.6787, 2.1459, 0.7538, 1.1382, 2.2902])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at jhu-clsp/mmBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_15819/71275596.py:222: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/tmp/ipykernel_15819/71275596.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `OptionalWeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using model: jhu-clsp/mmBERT-base\n",
      "Using Trainer class: OptionalWeightedTrainer\n",
      "CUDA available: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8352' max='8352' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8352/8352 19:14, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.175300</td>\n",
       "      <td>0.560622</td>\n",
       "      <td>0.829411</td>\n",
       "      <td>0.816945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.671000</td>\n",
       "      <td>0.505088</td>\n",
       "      <td>0.856775</td>\n",
       "      <td>0.844843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.196700</td>\n",
       "      <td>0.676197</td>\n",
       "      <td>0.868032</td>\n",
       "      <td>0.857457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.796513</td>\n",
       "      <td>0.872103</td>\n",
       "      <td>0.863108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1044' max='1044' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1044/1044 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7965131998062134, 'eval_accuracy': 0.872103466858272, 'eval_f1_macro': 0.8631084257571701, 'eval_runtime': 19.4692, 'eval_samples_per_second': 857.815, 'eval_steps_per_second': 53.623, 'epoch': 4.0}\n",
      "Saved metrics to: experiments/mmBERT-base_20260122_133831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 66800/66800 [00:00<00:00, 439548.98 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 16701/16701 [00:00<00:00, 402137.13 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model to: experiments/mmBERT-base_20260122_133831/best_model\n",
      "OUT_DIR: experiments/mmBERT-base_20260122_135823\n",
      "Train shape: (66800, 2)\n",
      "Validation shape: (16701, 2)\n",
      "Number of classes: 27\n",
      "Dataset({\n",
      "    features: ['text_stripped', 'label'],\n",
      "    num_rows: 66800\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text_stripped', 'label'],\n",
      "    num_rows: 16701\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 66800/66800 [00:05<00:00, 13132.97 examples/s]\n",
      "Map: 100%|██████████| 16701/16701 [00:01<00:00, 13174.45 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 66800\n",
      "})\n",
      "dict_keys(['label', 'input_ids', 'attention_mask'])\n",
      "Example label: 6\n",
      "Example input_ids length: 20\n",
      "num_labels: 27\n",
      "example: [(0, '10'), (1, '1140'), (2, '1160'), (3, '1180'), (4, '1280')]\n",
      "Class weights: tensor([0.6013, 0.7012, 0.4737, 2.4514, 0.3862, 0.9178, 0.3740, 2.6988, 0.7883,\n",
      "        0.5810, 0.3739, 0.4583, 2.3330, 0.3853, 2.2763, 0.3933, 0.3931, 1.3302,\n",
      "        0.3783, 0.7467, 0.1908, 0.7603, 0.6787, 2.1459, 0.7538, 1.1382, 2.2902])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at jhu-clsp/mmBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_15819/71275596.py:222: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/tmp/ipykernel_15819/71275596.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `OptionalWeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using model: jhu-clsp/mmBERT-base\n",
      "Using Trainer class: OptionalWeightedTrainer\n",
      "CUDA available: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8352' max='8352' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8352/8352 19:27, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.057400</td>\n",
       "      <td>0.490745</td>\n",
       "      <td>0.851745</td>\n",
       "      <td>0.840072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.621300</td>\n",
       "      <td>0.445255</td>\n",
       "      <td>0.877073</td>\n",
       "      <td>0.866489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.178400</td>\n",
       "      <td>0.636092</td>\n",
       "      <td>0.881624</td>\n",
       "      <td>0.871982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.739875</td>\n",
       "      <td>0.887552</td>\n",
       "      <td>0.877240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1044' max='1044' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1044/1044 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7398750185966492, 'eval_accuracy': 0.8875516436141548, 'eval_f1_macro': 0.8772400411732588, 'eval_runtime': 19.9769, 'eval_samples_per_second': 836.016, 'eval_steps_per_second': 52.26, 'epoch': 4.0}\n",
      "Saved metrics to: experiments/mmBERT-base_20260122_135823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 66800/66800 [00:00<00:00, 307706.97 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 16701/16701 [00:00<00:00, 530928.18 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model to: experiments/mmBERT-base_20260122_135823/best_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'run_name': 'mmBERT_lr2e-05_bs16_len16',\n",
       "  'eval_f1_macro': 0.8072690333763385,\n",
       "  'eval_loss': 1.0251343250274658,\n",
       "  'eval_accuracy': 0.816897191784923,\n",
       "  'best_model_checkpoint': 'experiments/mmBERT-base_20260122_131611/checkpoint-8352',\n",
       "  'output_dir': None},\n",
       " {'run_name': 'mmBERT_lr2e-05_bs16_len32',\n",
       "  'eval_f1_macro': 0.8631084257571701,\n",
       "  'eval_loss': 0.7965131998062134,\n",
       "  'eval_accuracy': 0.872103466858272,\n",
       "  'best_model_checkpoint': 'experiments/mmBERT-base_20260122_133831/checkpoint-8352',\n",
       "  'output_dir': None},\n",
       " {'run_name': 'mmBERT_lr2e-05_bs16_len64',\n",
       "  'eval_f1_macro': 0.8772400411732588,\n",
       "  'eval_loss': 0.7398750185966492,\n",
       "  'eval_accuracy': 0.8875516436141548,\n",
       "  'best_model_checkpoint': 'experiments/mmBERT-base_20260122_135823/checkpoint-8352',\n",
       "  'output_dir': None}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for exp in EXPERIMENTS:\n",
    "    CFG = make_cfg(BASE_CFG, exp)\n",
    "    out = run_experiment(CFG)   # you define this from your existing cells\n",
    "    results.append({\"run_name\": CFG[\"run_name\"], **out})\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
